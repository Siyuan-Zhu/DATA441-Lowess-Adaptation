{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Adaptation of LOWESS with surpport for multi-dimensional features and different kernels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iQXnfobbsLZf"
      },
      "outputs": [],
      "source": [
        "# Packages\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from math import ceil\n",
        "from scipy import linalg\n",
        "from scipy.interpolate import interp1d\n",
        "from scipy.linalg import lstsq\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.decomposition import PCA\n",
        "from scipy.spatial import Delaunay\n",
        "from sklearn.linear_model import LinearRegression, Ridge, ElasticNet\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import scipy.stats as stats\n",
        "from sklearn.model_selection import train_test_split as tts, KFold\n",
        "from sklearn.metrics import mean_squared_error as mse\n",
        "from scipy.interpolate import interp1d, griddata, LinearNDInterpolator, NearestNDInterpolator\n",
        "import statsmodels.api as sm\n",
        "from math import ceil\n",
        "from IPython.display import Image\n",
        "from IPython.display import display\n",
        "plt.style.use('seaborn-white')\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        "mpl.rcParams['figure.dpi'] = 130\n",
        "# the following line(s) are necessary if you want to make SKlearn compliant functions\n",
        "from sklearn.base import BaseEstimator, RegressorMixin\n",
        "from sklearn.utils.validation import check_X_y, check_array, check_is_fitted"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.5/34.5 MB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade --q scipy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b1QG37ElsvWh"
      },
      "outputs": [],
      "source": [
        "scale = StandardScaler()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Km7bwpA41Hdc"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv('drive/MyDrive/Colab Notebooks/data441/cars.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j3Y8J5GJ1SZc"
      },
      "outputs": [],
      "source": [
        "x = data.loc[:,'CYL':'WGT'].values\n",
        "y = data['MPG'].values\n",
        "xi = [[   4.,   79., 2074.],[   4.,   88., 2065.]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aqLYIXER1Sip"
      },
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "w5kt43gWaWRK"
      },
      "source": [
        "# The original:\n",
        "Original approach based on the following papers:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        ">William S. Cleveland: \"Robust locally weighted regression and smoothing\n",
        "scatterplots\", Journal of the American Statistical Association, December 1979,\n",
        "volume 74, number 368, pp. 829-836.\n",
        "\n",
        "\n",
        ">William S. Cleveland and Susan J. Devlin: \"Locally weighted regression: An\n",
        "approach to regression analysis by local fitting\", Journal of the American\n",
        "Statistical Association, September 1988, volume 83, number 403, pp. 596-610.\n",
        "\"\"\"\n",
        "##inspired by Alexandre Gramfort's univariate implementation \n",
        "\n",
        "original version of Gramfort's implementation of LOWESS: https://gist.github.com/agramfort/850437\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "va-kR5xZtZWi"
      },
      "outputs": [],
      "source": [
        "# Implementation by Alex Gramfort\n",
        "\n",
        "def lowess_ag(x, y, f=2. / 3., iter=3):\n",
        "    \"\"\"lowess(x, y, f=2./3., iter=3) -> yest\n",
        "    Lowess smoother: Robust locally weighted regression.\n",
        "    The lowess function fits a nonparametric regression curve to a scatterplot.\n",
        "    The arrays x and y contain an equal number of elements; each pair\n",
        "    (x[i], y[i]) defines a data point in the scatterplot. The function returns\n",
        "    the estimated (smooth) values of y.\n",
        "    The smoothing span is given by f. A larger value for f will result in a\n",
        "    smoother curve. The number of robustifying iterations is given by iter. The\n",
        "    function will run faster with a smaller number of iterations.\n",
        "    \"\"\"\n",
        "    n = len(x) \n",
        "    r = int(ceil(f * n)) # decies how many point around the Xi is important\n",
        "    # computes the smallest integer that is greater than or equal to f * n\n",
        "    # multiply faraction with num obs, and truncating it and force it to int\n",
        "    # instead of \"tau\" to control width of kernel, it controls the number of observations that's important(that's why it is foced int)\n",
        "    \n",
        "    h = [np.sort(np.abs(x - x[i]))[r] for i in range(n)]\n",
        "    # sort the observatio based on the distance to Xi, only applys to univariant\n",
        "    # iterates through X, get the distant of Xi to other points and and get the r th nearest neighbor\n",
        "    # \"r\" number of observations nearest to Xi forms the neighborhood, the others are clips in next line\n",
        "\n",
        "    w = np.clip(np.abs((x[:, None] - x[None, :]) / h), 0.0, 1.0) \n",
        "    # trick to creating square matrix of weights\n",
        "    # cuts off values(values) other than the number of neighbor needed \n",
        "    #(will get 1s for the ones not needed, but tricubic kernels turns 1s to 0s)\n",
        "    # w here contains the weights that will be used for lowess\n",
        "    # he claims his approach has some form of built in robustification\n",
        "    \n",
        "    # to adapt it to higher dimention(here is univariant), \n",
        "    # will need to change .abs to ecludian diatance\n",
        "    # also the trickery will not work\n",
        "\n",
        "    w = (1 - w ** 3) ** 3  #-->tricubic kernel, can try different\n",
        "         \n",
        "    # introdues form of \"rewighting\" of weights\n",
        "    # applying tricubic kernel to the weights\n",
        "\n",
        "    # for multi dimensional, will need to change the above lines to something more like KNN\n",
        "    # faiss: compute the k-nearest neighbors, or something from sklearn\n",
        "    # https://github.com/facebookresearch/faiss\n",
        "    # will also need to replace the kernels, Epanechnikov/gaussian(slower) for example\n",
        "    # need to rewrite the lines h,w, which gets the neigborhood of k, which is a fraction of total number of data\n",
        "    # and gets the weights\n",
        "\n",
        "    # first goal would be given any multi-dimentional x,\n",
        "    # if given a point x, how can i find the k-nearest neighbors?\n",
        "\n",
        "    yest = np.zeros(n)\n",
        "    delta = np.ones(n) # this creates adaptiveness\n",
        "    for iteration in range(iter): # iter to apply rounds of readjusting of weights\n",
        "        for i in range(n): # forces intercept no matter what\n",
        "            weights = delta * w[:, i] # delta will just be 1\n",
        "            b = np.array([np.sum(weights * y), np.sum(weights * y * x)])\n",
        "            A = np.array([[np.sum(weights), np.sum(weights * x)], \n",
        "                          [np.sum(weights * x), np.sum(weights * x * x)]])\n",
        "            beta = linalg.solve(A, b)  # this would be replaced as well\n",
        "\n",
        "            yest[i] = beta[0] + beta[1] * x[i] # this makes the linear prediction based on lm regression\n",
        "\n",
        "        residuals = y - yest # keeps the residuals\n",
        "        s = np.median(np.abs(residuals)) # take median of residuals\n",
        "        delta = np.clip(residuals / (6.0 * s), -1, 1) \n",
        "        # \"standardize\" the residuals, also cliping(discarding) residual outliers it based on magnitutde(-1,1)\n",
        "        # \"clips\" the residuals that are too large or small\n",
        "        # residuals that are bigger will get bigger weights, \n",
        "        # then those points will be given more importance(more influential)\n",
        "        # re-adjustes the original weight\n",
        "        \n",
        "        delta = (1 - delta ** 2) ** 2\n",
        "\n",
        "    return yest"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7SJ3bdR9b7Zn"
      },
      "source": [
        "# In order for this approach to be able to apply to multi-dimensional data, we need to adjust the way weights are calculated.\n",
        "\n",
        "In the original adaptation code:<br><br>\n",
        "**h = [np.sort(np.abs(x - x[i]))[r] for i in range(n)]**<br>\n",
        "**w = np.clip(np.abs((x[:, None] - x[None, :]) / h), 0.0, 1.0)**\n",
        "<br>\n",
        "> This univariate implementation takes the adsolute differernce between the center point x and all other points(its neighbors) as distance, sorts it in increaseing order, and selects the top $f * n$, or $f$ fraction amount of data points that are closest to center point x to form a \"neighborhood(or to \"localize\")\n",
        "\n",
        ">Then, it applies the \"tricubic\" kernel transformation to form the weights.<br>\n",
        "<br>\n",
        "**w = (1 - w ** 3) ** 3**\n",
        "<br>\n",
        "$$ K(x):=\\begin{cases}\n",
        "(1-\\|x\\|^3)^3 \\;\\;\\;if \\;\\;\\; \\|x\\|<1 \\\\\n",
        "0 \\;\\;\\; \\text{otherwise}\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "### **Other kernel functions such as 'gaussian' , 'epanechnikov' , 'quartic' also useable"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N3Fv0vHe_L-D"
      },
      "source": [
        "## Changes for the adapted vectorized version:\n",
        "\n",
        "\n",
        "**find_neighbors_tricubic(x, xi, n):**\n",
        ">This function fines the ecludian distances between centered point $x_i$ and all other points,finds the **$n$ nearest neighbors** and applies the **Tricubic** kernel transformation to form the weights $w$\n",
        "\n",
        "**weights_matrix_neighbors(x,x_new,kern,n):**\n",
        ">This function goes through every point in matrix X, and forms the **weights matrix** by applying the given kernel to each point\n",
        "\n",
        "## This version also predicts the data base on linear-interpolation\n",
        "\n",
        "\n",
        "> When training, the algorithm makes prediction for everything point in X, then makes lines or hyperplanes(linera-interpolatioin) in order to make predictions on new points\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rq26iiC7jk9T"
      },
      "outputs": [],
      "source": [
        "pca = PCA(n_components=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fjjqaI7KU5M2"
      },
      "source": [
        "## For X that has higher dimensions, drawing linear interpolations can be very slow, we can use PCA for dimensionality reduction before making interpolation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QxfIdC1CCOhj"
      },
      "outputs": [],
      "source": [
        "def lowess_regressor(X, y,x_new, kernel_type = 'tricubic',f=2. / 3., iter=3,intercept =True):\n",
        "    #kernels and weight calculation functions\n",
        "    # we need to compute all pairwise distances and figure out the k-nearest neightbors\n",
        "    def tricubic_neighbors(x, xi, n):\n",
        "\n",
        "        distances = np.sort(np.sqrt(np.sum((x - xi)**2, axis=1))) # axis = 1 means we are  calculating it column-wise\n",
        "        max = distances[n]\n",
        "        w = np.where(distances>max,0,70/81*(1-distances**3)**3)\n",
        "        return w   \n",
        "    def gaussian_neighbors(x, xi, n):\n",
        "\n",
        "        distances = np.sort(np.sqrt(np.sum((x - xi)**2, axis=1)))\n",
        "        max = distances[n]\n",
        "        w = np.where(distances>max,0,1/(np.sqrt(2*np.pi))*np.exp(-1/2*distances**2))\n",
        "        return w\n",
        "    def epanechnikov_neighbors(x, xi, n):\n",
        "\n",
        "        distances = np.sort(np.sqrt(np.sum((x - xi)**2, axis=1)))\n",
        "        max = distances[n]\n",
        "        w = np.where(distances>max,0,3/4*(1-distances**2))\n",
        "        return w\n",
        "    def quartic_neighbors(x, xi, n):\n",
        "\n",
        "        distances = np.sort(np.sqrt(np.sum((x - xi)**2, axis=1)))\n",
        "        max = distances[n]\n",
        "        w = np.where(distances>max,0,15/16*(1-distances**2)**2)\n",
        "        return w\n",
        "\n",
        "    def weights_matrix_neighbors(x,x_new,kernel,n):\n",
        "        if x_new.shape == 1:\n",
        "            x_new=x_new.reshape(1,-1)\n",
        "        length = len(x_new)\n",
        "        return np.array([kernel(x,x_new[i],n) for i in range(length)])\n",
        "    \n",
        "    if kernel_type in ['tricubic','gaussian','epanechnikov','quartic']:\n",
        "        if kernel_type == 'tricubic':\n",
        "            kernel = tricubic_neighbors\n",
        "        if kernel_type == 'gaussian':\n",
        "            kernel = gaussian_neighbors  \n",
        "        if kernel_type == 'epanechnikov':\n",
        "            kernel = epanechnikov_neighbors    \n",
        "        if kernel_type == 'quartic':\n",
        "            kernel = quartic_neighbors\n",
        "    else:\n",
        "        raise ValueError(\"kernel type must be 'tricubic','gaussian','epanechnikov' or 'quartic' \")\n",
        "    X, x_new, y =np.array(X), np.array(x_new), np.array(y)\n",
        "\n",
        "\n",
        "    length = len(X) # get the length of x\n",
        "    r = int(ceil(f * length)) # decies how many point around the Xi is important\n",
        "\n",
        "    if len(y.shape)==1: # here we make column vectors\n",
        "        y = y.reshape(-1,1)\n",
        "\n",
        "    if len(X.shape)==1:\n",
        "        X = X.reshape(-1,1)\n",
        "        \n",
        "    if intercept:\n",
        "        X1 = np.column_stack([np.ones((len(X),1)),X])\n",
        "    else:\n",
        "        X1 = X\n",
        "\n",
        "        # calculates the weights \n",
        "    w = weights_matrix_neighbors(X,X,kernel,r)\n",
        "#this variant is predicting on the train\n",
        "#the predict function uses linear interpolatioin to predict\n",
        "\n",
        "    yest = np.zeros(length)\n",
        "    delta = np.ones(length) # this creates adaptiveness\n",
        "\n",
        "    for iteration in range(iter): # iter to apply rounds of readjusting of weights\n",
        "        for i in range(length): # forces intercept no matter what\n",
        "            W = np.diag(delta)* np.diag(w[:,i]) \n",
        "            b = np.transpose(X1).dot(W).dot(y)\n",
        "            A = np.transpose(X1).dot(W).dot(X1)\n",
        "            A = A + 0.001*np.eye(X1.shape[1]) # if we want L2 regularization\n",
        "            #theta = linalg.solve(A, b) # A*theta = b\n",
        "            beta, res, rnk, s = lstsq(A, b)\n",
        "            yest[i] = np.dot(X1[i],beta)\n",
        "        \n",
        "        residuals = y- yest # keeps the residuals\n",
        "\n",
        "        s = np.median(np.abs(residuals)) # take median of residuals\n",
        "\n",
        "        delta = np.clip(residuals / (6.0 * s), -1, 1) \n",
        "            # \"standardize\" the residuals, also cliping(discarding) residual outliers it is is greater than 6 * median\n",
        "            # residuals that are bigger will get bigger weights, \n",
        "            # then those points will be given more importance(more influential)\n",
        "            # re-adjustes the original weight\n",
        "            # how robustification is applied\n",
        "            \n",
        "        delta = (1 - delta ** 2) ** 2\n",
        "        \n",
        "        \n",
        "    if X.shape[1]==1:\n",
        "        f = interp1d(x.flatten(),yest,fill_value='extrapolate')\n",
        "        output = f(x_new)\n",
        "    else:\n",
        "        output = np.zeros(len(x_new))\n",
        "        for i in range(len(x_new)):\n",
        "            ind = np.argsort(np.sqrt(np.sum((X-x_new[i])**2,axis=1)))[:r]\n",
        "            # the trick below is to have the Delaunay triangulation work --> getting first 3 PCA principle components\n",
        "            #(often without this delaunay would be infinite loop), bypass deficiency in delaunay function\n",
        "            pca = PCA(n_components=3)\n",
        "            X_pca = pca.fit_transform(X[ind])\n",
        "            tri = Delaunay(X_pca,qhull_options='QJ')\n",
        "            f = LinearNDInterpolator(tri,y[ind]) \n",
        "            output[i] = f(pca.transform(np.array(x_new[i]).reshape(1,-1))) # the output may have NaN's where the data points from xnew are outside the convex hull of X \n",
        "    if sum(np.isnan(output))>0:\n",
        "        g = NearestNDInterpolator(X,y.ravel()) \n",
        "        # output[np.isnan(output)] = g(X[np.isnan(output)])\n",
        "        output[np.isnan(output)] = g(x_new[np.isnan(output)])\n",
        "\n",
        "    return output"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "qbae8k90T-g8"
      },
      "source": [
        "## Create a class that could be used in sk-learn's syntax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D4dm8STw5Kta"
      },
      "outputs": [],
      "source": [
        "class Lowess_AG_adapted:\n",
        "    def __init__(self, f = 1/10, kernel_type = 'tricubic',iter = 3,intercept=True):\n",
        "        self.f = f\n",
        "        self.iter = iter\n",
        "        self.intercept = intercept\n",
        "        self.kernel_type = kernel_type\n",
        "    \n",
        "    def fit(self, x, y):\n",
        "        f = self.f\n",
        "        iter = self.iter\n",
        "        kernel_type = self.kernel_type\n",
        "        self.xtrain_ = x\n",
        "        self.yhat_ = y\n",
        "\n",
        "    def predict(self, x_new):\n",
        "        check_is_fitted(self)\n",
        "        x = self.xtrain_\n",
        "        y = self.yhat_\n",
        "        f = self.f\n",
        "        iter = self.iter\n",
        "        intercept = self.intercept\n",
        "        kernel_type = self.kernel_type\n",
        "        return lowess_regressor(x, y, x_new, kernel_type,f, iter, intercept)\n",
        "\n",
        "    def get_params(self, deep=True):\n",
        "    # suppose this estimator has parameters \"f\", \"iter\" and \"intercept\"\n",
        "        return {\"f\": self.f, \"iter\": self.iter,\"intercept\":self.intercept, \"kernel_type\":self.kernel_type}\n",
        "\n",
        "    def set_params(self, **parameters):\n",
        "        for parameter, value in parameters.items():\n",
        "            setattr(self, parameter, value)\n",
        "        return self"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3ainXpOWWUM"
      },
      "source": [
        "## Simple testing withAutomobile data from UCI\n",
        "https://archive.ics.uci.edu/ml/datasets/automobile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "16AoQZLpC8I5"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv('drive/MyDrive/Colab Notebooks/data441/cars.csv')\n",
        "\n",
        "x = data.loc[:,'CYL':'WGT'].values\n",
        "y = data['MPG'].values\n",
        "\n",
        "x_scaled = scale.fit_transform(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RqRDwfQi_LWX"
      },
      "outputs": [],
      "source": [
        "# k-fold validation\n",
        "def validation_function(x,y,model):\n",
        "  kf = KFold(n_splits=10,shuffle=True,random_state=123)\n",
        "  mse_test_lowess = []\n",
        "  for idxtrain, idxtest in kf.split(x):\n",
        "    xtrain = scale.fit_transform(x[idxtrain])\n",
        "    xtest = scale.transform(x[idxtest])\n",
        "    ytrain = y[idxtrain]\n",
        "    ytest = y[idxtest]\n",
        "    # for our 1-dimensional input data we do not need scaling\n",
        "    model.fit(xtrain,ytrain)\n",
        "    mse_test_lowess.append(mse(ytest,model.predict(xtest)))\n",
        "  return np.mean(mse_test_lowess)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NPLZPDDvWN6b",
        "outputId": "d008b8cb-8a5f-4cef-ef1f-6bc2f7a3d2c3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "26.614787944873125"
            ]
          },
          "execution_count": 163,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "validation_function(x,y,Lowess_AG_adapted())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jnzzkACmWpNu"
      },
      "source": [
        "# Using Grid search CV from sk-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aht9B2ZeAhyO",
        "outputId": "1b4dc315-bfe4-4c72-82d4-2b905460ead1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'lwr__f': 0.3333333333333333, 'lwr__iter': 1}"
            ]
          },
          "execution_count": 160,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import train_test_split as tts, KFold, GridSearchCV\n",
        "lwr_pipe = Pipeline([('zscores', StandardScaler()),\n",
        "                     ('lwr', Lowess_AG_adapted())])\n",
        "params = [{'lwr__f': [1/i for i in range(3,15)],\n",
        "         'lwr__iter': [1,2,3,4]}]\n",
        "\n",
        "\n",
        "gs_lowess = GridSearchCV(lwr_pipe,\n",
        "                      param_grid=params,\n",
        "                      scoring='neg_mean_squared_error',\n",
        "                      cv=5)\n",
        "gs_lowess.fit(x, y)\n",
        "gs_lowess.best_params_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4kPX3UmiSVWA",
        "outputId": "192645ee-098a-4ca0-bb18-9b3c643a50ee"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "-29.934237859286906"
            ]
          },
          "execution_count": 161,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "gs_lowess.best_score_"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
